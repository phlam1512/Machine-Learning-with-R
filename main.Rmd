---
title: "ST310 Project"
---
# Introduction

## Description of dataset
This dataset contains characteristics of customers of a telecommunications company, such as age, gender, the services they currently use with the company, payment methods and monthly charges, as well as the outcome variable Churn.

The outcome variable Churn, is a binary variable (Yes/No) indicating whether the customer has churned (stopped using the services provided by this telecommunication company). By "learning" the characteristics of the customers who have churned, the company will be able to predict whether their customers are likely to churn, based on historical data, as well as to predict whether a new customer who has such characteristics is likely to stay with the company.

Predicting the outcome makes sense for the telecommunication company since it is in the company's interest to retain their customers. The developed models could also inform their future strategies when seeking to expand their customer base.

```{r Setup, cache=TRUE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(glmnet) #Logistic regression
library(tree) 
library(randomForest)
library(caret) #Confusion matrix
set.seed(1) #To keep results consistent 
```

# Preparing the data

## Importing the data
```{r Importing the data}
#Import dataset from CSV
df <- read.csv("dataset.csv", head(T))

#Check data formats of each column
str(df)
```

## Data clean-up

Several rows are missing values for the `TotalCharges` column. As a result, we decided to omit them from our dataset. There were 11 such observations, as can be seen below.
```{r Data clean-up}
#Find rows with missing values
df[!complete.cases(df),]
#Exclude rows with missing values - 11 out of about 7000
df <- na.omit(df)
```

## Convert data formats

Here, we first removed the `customerID` column as we are not interested in these values to make our predictions.

We then separated the different levels of our qualitative predictors (or factors) by selecting all variables with values in the character (char) format. We had one more factor `SeniorCitizen` that was not in the char format, and hence also converted this manually.

```{r Convert format, cache=TRUE}
#Remove first column since this is a unique identifier for each customer
df <- subset(df,select=-customerID)
#Convert all columns which are formatted as character to factor
df <- df %>%
  mutate_if(is.character,as.factor)
#Convert SeniorCitizen to factor as although this is also a factor, it was coded as integers 0 and 1 in the original dataset.
df$SeniorCitizen <- as.factor(df$SeniorCitizen)

#Verify converted data formats are as expected
str(df)
```

## Splitting the dataset

The next step was to split our dataset into our training and test datasets. We used 3/4 of our observations for the training data, and the remaining for the test data.

```{r Split data, cache=TRUE}
#Stratified sampling
df_split <- initial_split(df, prop=3/4, strata=Churn)
df_split

df_recipe <- training(df_split) %>%
  recipe(Churn ~ .) %>%
  prep()

df_testing <- df_recipe %>%
  bake(testing(df_split))
df_training <- juice(df_recipe)

#Verify proportion of Churn customer is indeed preserved in both training and testing sets
df_training %>%
  count(Churn) %>%
  mutate(prop = n/sum(n))
df_testing %>%
  count(Churn) %>%
  mutate(prop = n/sum(n))
```

## Creating the matrix

```{r Tidymodels workflow, cache=TRUE}
df_recipe <- training(df_split) %>%
  recipe(Churn ~ .) %>%
  prep()

df_testing <- df_recipe %>%
  bake(testing(df_split))
df_training <- juice(df_recipe)
```

# Baseline - Models 1 & 2: Logistic Regression and Gradient Descent

## Variable Selection

Here, we performed both forward selection (FS) and backward elimination (BE) to try and narrow down the set of predictor variables that we would include in our first two models below (logistic regression and gradient descent). We note that the model generated by both FS and BE algorithms are in agreement, i.e. yielded the same model.

A number of columns are linearly dependent, since if a customer did not have `InternetService`, it would also not have `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`, `StreamingTV` or `StreamingMovies`. More specifically, the dummy variables for each of the above for those with "No internet service" would be the exact same vectors. The table below includes the number of customers in our data who did not have Internet service, for which they are many of them (just under 30%) so we cannot simply remove these observations, unlike previously where we have done so for rows that have missing data as that number is much lower.

```{r Table of NoInternet, cache=TRUE}
table(df$InternetService)
```

Thus, when performing forward selection or backward elimination, the matrix including the data points of such dummy variables (and possibly other variables/predictors) would be non-invertible, and hence there would be no unique optimal solution. This explains the NA for these dummy variables in the resulting outputs below, and as both logistic regression and gradient descent involve finding the inverse of such a matrix, we would not be able to continue with both. We thus removed these predictors from our dataset when proceeding with the computation of both these models below.

We also decided to omit `gender`, `Partner`, `PhoneService`, `MonthlyCharges` and `TotalCharges`, as the forward selection deemed these variables to be relatively insignificant for the prediction of Churn.
```{r FS and BE, cache=TRUE}
#Full model
Full <- glm(Churn ~ ., data=df_training, family="binomial")
#Null model
Null <- glm(Churn ~ 1, data=df_training, family="binomial")

#Mask step function from stats instead of tidymodels
step <- stats::step

#Backward elimination
BE <- step(Full, scope=list(lower=Null, upper=Full), direction="backward", trace=F)
summary(BE)

#Forward selection
FS <- step(Null, scope=list(lower=Null, upper=Full), direction="forward", trace=F)
summary(FS)
```

## Logistic regression

For our baseline model, we performed logistic regression.

### Implementation
```{r Logistic, cache=TRUE}
model_logistic <- logistic_reg(mode="classification") %>%
  set_engine("glm") %>%
  fit(Churn ~ SeniorCitizen+Dependents+tenure+MultipleLines+InternetService+Contract+PaperlessBilling+PaymentMethod, data=df_training)
```

The coefficients from the model suggest that whether a customer uses a fiber optic internet service is the variable that contributes most to the probability of a customer churning. 
```{r Logistic model output, cache=TRUE}
model_logistic
```

### Analysis
The resulting model had an accuracy rate of approximately 80% on our test data, and as expected testing error is slightly, although not significantly lower than the testing error.

```{r Training and testing error, cache=TRUE}
#Training error
model_logistic %>%
  predict(df_training) %>%
  bind_cols(df_training) %>%
  metrics(truth=Churn, estimate=.pred_class)

#Testing error
model_logistic %>%
  predict(df_testing) %>%
  bind_cols(df_testing) %>%
  metrics(truth=Churn, estimate=.pred_class)
```

## Gradient Descent

Next, we performed a gradient descent on the same variables as in logistic regression above.

### Implementation
```{r Gradient descent algorithm, cache=TRUE}
#Convert the dataframe into matrix - convert factors into dummy variables
#Same predictor variables used here as in logistic regression in previous sections
x <- model.matrix(Churn ~ SeniorCitizen+Dependents+tenure+MultipleLines+InternetService+Contract+PaperlessBilling+PaymentMethod, data=df_training)
y <- ifelse(df_training["Churn"]=="Yes",1,0)

# Loss function of logistic regression
Loss <- function(X,Y,beta){
  -(1/nrow(Y)) * sum((Y)*log((1/(1+exp(-x %*% beta)))) + (1-Y)*log(1-(1/(1+exp(-x %*% beta)))))
}

#Gradient = First derivative with respect to beta of Loss function
Loss_grad <- function(X,Y,beta){
  (1/nrow(Y)) * (t(X) %*% ((1/(1+exp(-x %*% beta))) - Y))
}

#Initialise the algorithm
p <- ncol(x)
start_beta <- rep(0,p) #Starting at the origin
current_loss <- Loss(x,y,start_beta)
start_grad <- Loss_grad(x,y,start_beta)
next_beta <- start_beta - 0.01 * start_grad 
current_beta <- next_beta
current_grad <- start_grad
next_loss <- Loss(x,y,current_beta)
steps <- 0 #Counter

# Below, the number of steps was chosen to be 200,000 and the step-size to be 0.01. This was a result of experimentation with various values in a bid to keep computation manageable but also sufficiently accurate when compared to the results obtained from logistic regression.

while (steps<200000){
  next_grad <- Loss_grad(x,y,current_beta)
  
  # Moving the beta vector in the direction of the current gradient, resulting in a lower loss value. The current gradient value was normalized.
  next_beta <- current_beta - 0.01 * current_grad / sqrt(sum(current_grad^2)) 
  #Current_beta updated with the newly computed beta that gives a lower loss.
  current_beta <- next_beta 
  
  current_loss <- next_loss
  current_grad <- next_grad
  next_loss <- Loss(x,y,next_beta)
  steps <- steps+1 
  
  #To check progress of algorithm
  if (steps %% 1000 == 0) print(next_loss - current_loss) 
  if (steps %% 1000 == 0) print(steps)
}

#Gradient descent result
print(current_loss) 
print(current_beta) 
```

## Analysis

```{r , cache=TRUE}
#Training data
x_train <- x
y_train <- y
#Convert log-odds to probability of churn
logodds_train <- x_train %*% current_beta
odds_train <- exp(logodds_train)
prob_train <- odds_train/(1+odds_train)
#Decision threshold (to predict that customer churns) chosen to be 0.5 
y_hat_train <- ifelse(prob_train>=0.5, 1, 0)
outcome_train <- cbind(y_train,prob_train,y_hat_train)
outcome_train <- data.frame(outcome_train)
# Rename column headers
colnames(outcome_train) <- c("True Outcome", "Prob of Churn", "Predicted Outcome") 
# Sample output
head(outcome_train)
#Confusion matrix on training data
caret::confusionMatrix(as.factor(outcome_train$`Predicted Outcome`), as.factor(outcome_train$`True Outcome`))
```


```{r , cache=TRUE}
#Test data
x_test <- model.matrix(Churn ~ SeniorCitizen+Dependents+tenure+MultipleLines+InternetService+Contract+PaperlessBilling+PaymentMethod, data=df_testing)
y_test <- ifelse(df_testing["Churn"]=="Yes",1,0)
#Convert log-odds to probability of churn
logodds_test <- x_test %*% current_beta
odds_test <- exp(logodds_test)
prob_test <- odds_test/(1+odds_test)
#Decision threshold chosen to be 0.5
y_hat_test <- ifelse(prob_test>=0.5, 1, 0)
outcome_test <- cbind(y_test,prob_test,y_hat_test)
# Rename column headers
colnames(outcome_test) <- c("True Outcome", "Prob of Churn", "Predicted Outcome") 
# Sample output
head(outcome_test)
```

The predictive accuracy of our gradient descent model on our test data (approximately 79%) is slightly lower than that of our baseline.
```{r , cache=TRUE}
#Confusion matrix on testing data
caret::confusionMatrix(as.factor(outcome_train$`Predicted Outcome`), as.factor(outcome_train$`True Outcome`))
```

### Comparison Between Logistic Regression Model with Gradient Descent

The table below compares the coefficients obtained from both models. Very similar coefficients were obtained. However, we do note that if we let the gradient descent run for more steps, the results will be more accurate.

```{r , cache=TRUE}
compare_table <- cbind(tidy(model_logistic)[,2],current_beta)
colnames(compare_table) <- c("Logistic regression", "Gradient descent")
compare_table
```


# Relatively Interpretable - Model 3: Decision Tree

Next up, for our interpretable non-baseline model, we used a decision tree. It should be noted that all of the available predictors were available for selection for the tree (unlike with the previous two models). 

## Implementation

```{r , cache=TRUE}
#Fit the decision tree on training data
model_tree <- decision_tree(mode="classification") %>%
  set_engine("rpart") %>%
  fit(Churn ~ ., data=df_training)
model_tree
#Plot the decision tree
rpart.plot::rpart.plot(model_tree$fit, roundint = FALSE)
```

## Analysis

```{r , cache=TRUE}
#Training error
model_tree %>%
  predict(df_training) %>%
  bind_cols(df_training) %>%
  metrics(truth=Churn, estimate=.pred_class)

#Testing error
model_tree %>%
  predict(df_testing) %>%
  bind_cols(df_testing) %>%
  metrics(truth=Churn, estimate=.pred_class)
```
The model had a predictive accuracy of approximately 78%, which is less than for both of the previous models.


## Interpretation and Comparison with Baseline

### First-layer node: Contract
The tree predicts that customers on a contract (either one-year or two-year one) will not churn. This is consistent with what is suggested by the previous models. For example, with our logistic regression model, if we ignore all other predictors, we can compute that a customer on a one year contract compared to one not on any contract has a lower churn probability by approximately 16%. Similarly, a customer on a two year contract is about 27% less likely to churn than one not on a contract. This also aligns with the real-life idea that customers not on a contract are less committed to the services they use and hence are more likely to churn.
$$
\mathbb{P}(\text{Churn} \, | \, \text{No Contract}) = \frac{e^{0}}{1+e^{0}}=0.5
$$
$$
\mathbb{P}(\text{Churn} \, | \, \text{One Year Contract}) = \frac{e^{-0.6818}}{1+e^{-0.6818}}=0.3359
$$
$$
\mathbb{P}(\text{Churn} \, | \, \text{Two Year Contract}) = \frac{e^{-1.2320}}{1+e^{-1.2320}}=0.2258
$$

### Second-layer node: Internet Service
Secondly, we observe that customers using a fibre optic internet service are more likely to churn than those using DSL or no internet service, in agreement to the logistic regression model above (calculations shown below). In the tree, we predict that customers who are not on a contract and uses fibre optic internet to Churn, with probability 0.55, which is significantly lower than the probability predicted by the logistic regression model. On the other hand, customers who are not on a contract and do not use fibre optic internet (so either has DSL internet or no internet at all) are predicted to not churn (probability 0.28). This could suggest customer dissatisfaction with the company's fibre optic offering. Consequently, the company may want to investigate into this further and consider actions they can take to keep these customers.

$$
\mathbb{P}(\text{Churn} \, | \, \text{DSL Internet & No Contract}) = \frac{e^{0+0}}{1+e^{0+0}}=0.5
$$
$$
\mathbb{P}(\text{Churn} \, | \, \text{Fiber Optic Internet & No Contract}) = \frac{e^{1.1004}}{1+e^{1.1004}}=0.7503
$$
$$
\mathbb{P}(\text{Churn} \, | \, \text{No Internet & No Contract}) = \frac{e^{-0.7088}}{1+e^{-0.7088}}=0.3299
$$

### Third-layer node: Tenure
Customers with a shorter tenure are less likely to churn - those with a higher tenure were less likely to be predicted to churn. In particular, the tree predicted that fibre optic customers (who are not on a contract) with a low tenure (less than 14 months) will churn. This is also in alignment with real-world expectations regarding customer loyality, where customers who have stayed with the company for longer will be less likely to churn.

$$
\mathbb{P}(\text{Churn} \, | \, \text{Fiber Optic Internet & No Contract & Tenure} = 13) = \frac{e^{0+1.1004+13 \times -0.0354}}{1+e^{0+1.1004+13 \times -0.0354}}=0.6548
$$
$$
\mathbb{P}(\text{Churn} \, | \, \text{Fiber Optic Internet & No Contract & Tenure} = 14) = \frac{e^{0+1.1004+14 \times -0.0354}}{1+e^{0+1.1004+14 \times -0.0354}}=0.6468
$$
$$
\mathbb{P}(\text{Churn} \, | \, \text{Fiber Optic Internet & No Contract & Tenure} = 15) = \frac{e^{0+1.1004+15 \times -0.0354}}{1+e^{0+1.1004+15 \times -0.0354}}=0.6386
$$

### Lower-layer node: Payment Method
Those using an electronic check (instead of the other payment methods) were more likely to churn (56% verses 39%), given that the customer did not have a contract, use fibre optic internet, had a tenure of at least 14 months, and total charges less than 3084). Indeed, this matched with what the baseline model (logistic regression) suggested: ignoring all other predictors, a customer with an electronic check had the highest churn probability (58%) out of all four payment methods.

$$
\mathbb{P}(\text{Churn} \, | \, \text{Electronic Check}) = \frac{e^{0.3196}}{1+e^{0.3196}}=0.5792
$$
$$
\mathbb{P}(\text{Churn} \, | \, \text{Mailed Check}) = \frac{e^{-0.0369}}{1+e^{-0.0369}}=0.4908
$$
$$
\mathbb{P}(\text{Churn} \, | \, \text{Credit Card}) = \frac{e^{-0.1940}}{1+e^{-0.1940}}=0.4517
$$
$$
\mathbb{P}(\text{Churn} \, | \, \text{Bank Transfer}) = \frac{e^{0}}{1+e^{0}}=0.5
$$

# Relatively High-Dimensional and Accurate - Models 4 and 5: Random Forest
Our final model, a random forest, is both relatively high-dimensional and focused on predictive accuracy rather than interpretability.

## Implementation

```{r , cache=TRUE}
#Setup the workflow
rf_mod <-
  rand_forest(trees = 128) %>%
  set_engine("randomForest") %>%
  set_mode("classification")

#Fit random forest on training data
rf_fit <-
  rf_mod %>%
  fit(Churn ~ ., data=df_training)
rf_fit
```

## Without cross-validation
We initially fit the model on the entire training data set. We compute the training accuracy (here without cross-validation) and the testing accuracy. We note that the training accuracy is very good (98%) and much better than the training accuracy. However, to properly evaluate the performance of this model, we perform 10-fold cross-validation in the following chunk of code.

```{r RF no CV, cache=TRUE}
#Training data
rf_train_pred <-
  predict(rf_fit, df_training) %>%
  bind_cols(predict(rf_fit, df_training, type="prob")) %>%
  bind_cols(df_training %>%
              select(Churn))
#Training accuracy
rf_train_pred %>%
  accuracy(truth=Churn, .pred_class)
#Training ROC
rf_train_pred %>%
  roc_auc(truth=Churn, .pred_No)

#Testing data
rf_test_pred <-
  predict(rf_fit, df_testing) %>%
  bind_cols(predict(rf_fit, df_testing, type="prob")) %>%
  bind_cols(df_testing %>%
              select(Churn))
#Testing accuracy
rf_test_pred %>%
  accuracy(truth=Churn, .pred_class)
#Testing ROC
rf_test_pred %>%
  roc_auc(truth=Churn, .pred_No)

#Testing ROC curve
rf_test_pred %>%
  roc_curve(truth=Churn, .pred_No) %>%
  autoplot()
  
```

## With 10-fold cross-validation
Now with 10-fold cross-validation, we can more properly evaluate the actual performance of the Random Forest model. Here, we see that the accuracy from 10-fold cross validation on the training data is about 79%, which is similar to testing accuracy at 80%.

```{r RF with CV, cache=TRUE}
#Create the folds
folds <- vfold_cv(df_training, v=10)
folds

#Setup the workflow
rf_wf <-
  workflow() %>%
  add_model(rf_mod) %>%
  add_formula(Churn ~ .)

#Fit the random forest on the 10-folds
rf_fit_rs <-
  rf_wf %>%
  fit_resamples(folds)
rf_fit_rs

#Evaluate the model
collect_metrics(rf_fit_rs)
```

# Conclusion

```{r, cache=TRUE, echo=FALSE}
summary_logit <- c("Logistic Regression",0.8037, 0.7959)
summary_gd <- c("Gradient Descent",0.8041, 0.7959)
summary_tree <- c("Decision Tree",0.7978, 0.7902)
summary_rf <- c("Random Forest", 0.7925, 0.8022)

summary <- rbind(summary_logit, summary_gd, summary_tree, summary_rf)
colnames(summary) <- c("Model","Training Accuracy", "Testing Accuracy")
summary
```

All four models performed very similarly. The table above summarises the results. We note that the decision tree performed the worse, but not much worse than the other models (Logistic Regression and Random Forest). Here, we also observe that the Random Forest peformed the best for the testing set, but again not significantly better.

Trying to use more complex and high-dimensional models like the decision tree and random forest did not yield a lot of improvement compared to the relatively simpler logistic regression and gradient descent. However, we do note that the decision tree is the easiest to interpret as one can simply follow the branches of the tree to reach a prediction (rather than having to perform calculations, as with the other models). On the other hand, the random forest focused on predictive accuracy, but in exchange for being relatively difficult to interpret.  

Trying to use very complex models here may have led to a bit more overfitting to the training data. This may also be attributed to the way the training and testing data is drawn using the `initial_split` function and the fact that we have set a particular seed to keep results consistently the same while working with this dataset. Nonetheless, we have used stratified sampling as we expect the proportion of customers who Churn to remain roughly consistent.

The three models from which some interpretations could be made all yielded consistent interpretations as outlined earlier.

Overall, we would say the logistic regression model is the best among the models that we have used on this dataset, since it is computationaly inexpensive, relatively simple to interpret and understand how each of the variables contribute to the prediction. An improvement to the baseline model would be to consider interaction terms and higher order terms.

